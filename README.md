# PitCrew AI ğŸš¨ğŸï¸

### Autonomous SRE System Built with MCP-Orchestrated Agents

PitCrew AI is an autonomous Site Reliability Engineering (SRE) platform designed to detect, diagnose, and remediate production incidents safely using AI agents.

This project is being built as part of an advanced systems-focused hackathon with the goal of demonstrating **real-world AI infrastructure orchestration**, not just chatbot capabilities.

The architecture emphasizes:

* Control-plane vs Data-plane separation
* Safe automation
* Observability-driven decisions
* Governed remediation
* Production-style failure simulation

---

# ğŸ¯ Project Vision

Modern infrastructure demands intelligent automation â€” but unsafe autonomy can be catastrophic.

PitCrew AI aims to answer a critical question:

> **Can AI operate production systems safely?**

Instead of building a simple AI assistant, this project focuses on creating a **structured operational system** where agents:

1. Detect system failures
2. Investigate root causes
3. Consult operational knowledge
4. Validate actions against policy
5. Execute remediation
6. Generate incident reports

The long-term goal is to simulate a production-grade autonomous SRE.

---

# ğŸ§  Core Architectural Principle

## Control Plane vs Data Plane

This project intentionally separates system responsibilities:

### âœ… Data Plane (Workload)

The production system being monitored.

Currently includes:

* A dockerized Flask API
* Health monitoring endpoint
* Controlled failure triggers

### âœ… Control Plane (Operator)

The intelligence layer that observes and controls the system.

Currently includes:

* Chaos simulation script
* Docker command execution from host
* External system control

This mirrors real-world infrastructure patterns used by Kubernetes and cloud platforms.

---

# ğŸ—ï¸ What Was Built â€” Day 1 Foundation

Day 1 focused entirely on building a **realistic, controllable production environment**.

Before creating AI agents, it is critical to have a system that can:

âœ… fail predictably
âœ… recover reliably
âœ… expose health signals

Without this foundation, observability and remediation cannot be demonstrated convincingly.

---

# ğŸ³ Dockerized Production Service

A lightweight Flask API was containerized to act as the "production workload."

## Why Docker?

Docker ensures the service runs in a consistent environment across machines by packaging:

* Application code
* Dependencies
* Runtime
* OS layer

This eliminates the classic deployment issue:

> "It works on my machine."

---

## Container Behavior

### Healthy State

```
/health â†’ HTTP 200 OK
```

### Failed State

```
/health â†’ HTTP 500 SERVICE UNHEALTHY
```

Failure is triggered via a filesystem flag:

```
broken.flag
```

This allows deterministic outage simulation.

---

# ğŸ’¥ Chaos Engineering Setup

To simulate realistic production incidents, a control script was created:

## chaos.py

Runs on the **host machine**, not inside the container.

This is intentional.

### Why?

Production systems should never self-destruct.

Failures must be triggered externally â€” just like real infrastructure where operators or unexpected events impact services.

This enforces proper architectural separation:

### Control Plane â†’ manages

### Data Plane â†’ executes

---

## Chaos Capabilities

### Break the Service

Creates `broken.flag` inside the container, forcing the health endpoint to return 500.

### Restore the Service

Removes the flag and returns the system to healthy status.

---

## Example Flow

Simulate outage:

```
python chaos.py
> break
```

Restore service:

```
python chaos.py
> fix
```

This enables **one-command failure demos**, which are critical for reliable technical presentations.

---

# ğŸ“ Current Project Structure

```
pitcrew-ai/
â”‚
â”œâ”€â”€ victim-app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ chaos.py
â””â”€â”€ README.md
```

### victim-app

Represents the production workload.

### chaos.py

Represents the external operator capable of controlling the system.

---

# ğŸ§± Key Engineering Decisions

## Deterministic Failures

Random crashes are bad for demos.

Predictable failures enable reliable testing and presentation.

---

## Minimal Infrastructure

Kubernetes was intentionally avoided to reduce resource overhead and increase development velocity.

Docker provides sufficient realism without unnecessary complexity.

---

## Externalized Control

Automation scripts remain outside the container to reflect real platform architecture patterns.

---

# ğŸ”œ What Comes Next

With a controllable production system in place, the next phase introduces intelligent observability.

## Upcoming Component:

### ğŸ”§ Mechanic MCP Agent

Responsibilities:

* Inspect Docker containers
* Read logs
* Detect unhealthy services
* Surface diagnostic signals

This is where the system begins evolving from a container demo into an **AI-operated infrastructure platform.**

Future agents will include:

* Strategist â†’ Runbook-powered RAG
* Official â†’ Policy validation
* Risk Engine â†’ Action scoring
* Reporter â†’ Automated postmortems

---

# ğŸš€ Long-Term Architecture (Target)

```
Incident Trigger
      â†“
Observability Agent
      â†“
Diagnosis
      â†“
Runbook Retrieval
      â†“
Policy Validation
      â†“
Risk Assessment
      â†“
Autonomous Remediation
      â†“
Incident Report
```

The objective is not to create a chatbot, but a **governed autonomous operator.**

---

# ğŸ’¡ Why This Project Matters

AI is rapidly gaining operational authority inside production environments.

The challenge is no longer intelligence.

It is **trust.**

PitCrew AI explores how structured orchestration, policy enforcement, and risk-aware automation can make AI safe enough to operate critical systems.

---

# âœ… Day 1 Status

âœ” Dockerized production service
âœ” Health monitoring endpoint
âœ” Deterministic failure mechanism
âœ” Chaos simulation
âœ” Control/Data plane separation

## Day 2 â€” Autonomous Recovery Engine

Day 2 upgraded **PitCrew AI** from a failure simulator into an **autonomous self-healing system**.

It now runs a closed-loop workflow:

**Failure â†’ Diagnose â†’ Decide â†’ Remediate â†’ Verify**

aligned with real-world **SRE automation**.

---

### Architecture

PitCrew introduces a **data plane / control plane** split:

- **Data Plane:** Dockerized Flask workload with deterministic failure + health telemetry  
- **Control Plane:** Mechanic MCP (FastAPI) + AI Driver for observability and execution  

This keeps decisions external to workloads, like production infrastructure.

---

### Core Components

**Mechanic MCP (Ops Layer)**  
- Logs, inspect, restart, targeted recovery  
- Executes actions, never decides  

**AI Driver (Brain)**  
- Collects telemetry  
- Diagnoses via LLM  
- Normalizes outputs into deterministic actions  
- Executes remediation  
- Verifies recovery  

---

### Key Engineering Wins

- **Context engineering:** Only critical log signals go to the model  
- **Deterministic actions:** Free text â†’ stable commands  
- **Root-cause recovery:** Remove failure trigger + restart, not blind reboot  

---

### Result

PitCrew AI now performs **fully autonomous self-healing**:

Chaos â†’ Observe â†’ Reason â†’ Fix â†’ Verify  

with no human intervention, modeling real SRE behavior instead of a demo bot.

Foundation complete.

The system is now ready for intelligent observability.

---


## Day 3 â€” Governance Layer & Reliability Hardening

Day 3 focused on transforming PitCrew from an autonomous recovery script into a governed infrastructure system by introducing policy enforcement, execution safety, and state-aware incident validation.

### Key Architectural Upgrade
Implemented a Policy Engine ("Official") to validate all remediation actions before execution.  
This establishes a controlled workflow:

Health â†’ Diagnosis â†’ Policy Evaluation â†’ Approved Action â†’ Recovery â†’ Verification

The system now demonstrates governed autonomy rather than unrestricted AI-driven execution.

---

### Policy Engine Integration
- Built a dedicated FastAPI policy service.
- Enforced approval checks before container remediation.
- Introduced human-in-the-loop override for high-severity incidents.
- Prevented direct LLM-to-infrastructure execution.

This aligns the platform with real-world operational risk controls.

---

### Severity Normalization
Detected a governance bypass caused by vocabulary drift (`CRITICAL` vs `HIGH`).  
Implemented severity normalization to enforce a strict contract between the Driver and Policy Engine.

Result:
- Eliminated unintended auto-approvals.
- Strengthened decision determinism.

---

### Deterministic Parsing
Replaced fuzzy keyword detection with structured field extraction from LLM output.

Benefits:
- Reduced ambiguity in action selection.
- Improved automation reliability.
- Prevented prompt bleed from affecting execution.

---

### False Incident Prevention (Major Reliability Upgrade)
Observed that historical Docker logs triggered recovery on healthy services.

Added a **health pre-check gate** before running AI diagnosis:

Live Service State â†’ Validate â†’ Diagnose (only if degraded)

This shifted the system from log-driven behavior to state-aware incident response â€” a critical reliability pattern.

---

### Execution Safety Improvements
- Added guarded recovery flow to prevent duplicate remediation.
- Introduced safe request wrappers to handle service outages gracefully.
- Hardened driver against dependency failures.

The control plane now fails safely rather than unpredictably.

---

### Operational Traceability
Added incident IDs to each response cycle, improving observability and aligning the system with real incident-management workflows.

---

### Outcome
PitCrew now operates as a governed recovery platform with:

- Policy-based execution control  
- Human override for high-risk actions  
- Deterministic AI behavior  
- State-aware incident detection  
- Hardened orchestration layer  

This marks the transition from a prototype automation script to a reliability-oriented control plane.

feat(governance): add risk scoring, rule attribution, and audit trail
